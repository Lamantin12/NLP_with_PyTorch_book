{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e2ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6de7f",
   "metadata": {},
   "source": [
    "# Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd0d2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Класс, предназначенный для обработки текста и извлечения значений токенов\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): pre-existing map of tokens to indices\n",
    "            add_unk (bool): flag that indicates whether add the UNK token\n",
    "            unk_token (str): UNK token to add in Vocab\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {\n",
    "            idx: token \n",
    "            for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializable(self):\n",
    "        \"\"\" возвращает словарь с возможностью сериализации \"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self._unk_token\n",
    "        }\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" создает экземпляр класса Vocabulary из сериализованного словаря \"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Добавляет токен в словари, возвращая его индекс\n",
    "        \n",
    "        Args:\n",
    "            token (str): токен, добавляемый в Vocabulary\n",
    "        Returns:\n",
    "            index (int): индекс токена в словарях\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Добавляет список токенов в словарь\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): список токенов типа string\n",
    "        Returns:\n",
    "            indices (list): список индексов, соответствующих списку токенов\n",
    "        \"\"\"\n",
    "        return [self.add_token for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Возвращает число, соответствующее токену или индекс элемента UNK.\n",
    "        \n",
    "        Args:\n",
    "            token (str): токен\n",
    "        Returns:\n",
    "            index (int): индекс, соответствующий токену\n",
    "        Notes:\n",
    "            `unk_index` должен быть >=0 (добавлен в словарь) \n",
    "              для функционирования UNK\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"The index {index} is not in Vocabulary\")\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Vocabulary(size={len(self)})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbea072",
   "metadata": {},
   "source": [
    "# Vectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f62f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" Класс, координирующий Vocabularies и использует их\n",
    "    \"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): токены - цифры\n",
    "            rating_vocab (Vocabulary): метки классов - цифры\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "        \n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Создает вектор для обзора\n",
    "        Args:\n",
    "            review (str): обзор\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): one-hot вектор\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\"Инициализирует Vectorizer из pandas.DataFrame\n",
    "        \n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): датасет обзоров\n",
    "            cutoff (int): параметр для отсеивания по частоте\n",
    "        Returns:\n",
    "            Экземпляр Vectorizer\n",
    "        \"\"\"        \n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Инициализирует Vectorizer из сериализованного словаря\n",
    "        Args:\n",
    "            contents (dict): сериализованный словарь\n",
    "        Returns:\n",
    "            Экземпляр Vectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Создает сериализованный словарь на основе класса Vectorizer\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'review_vocab': self.review_vocab.to_serializable(),\n",
    "            'rating_vocab': self.rating_vocab.to_serializable()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2800b45",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ea32763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): Датасет\n",
    "            vectorizer (Vectorizer): Vectorizer, созданный из датасета\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.val_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"Загружает датасет из пути review_csv и создает для него Vectorizer\n",
    "        Args:\n",
    "            review_csv (str): путь к данным\n",
    "        Returns:\n",
    "            Экземпляр ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, Vectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\"Загружает датасет из пути review_csv и Vectorizer из пути vectorizer_filepath\n",
    "        Используется в случае если Vectorizer был закеширован\n",
    "        Args:\n",
    "            review_csv (str): путь к данным\n",
    "            vectorizer_filepath (str): путь к Vectorizer\n",
    "        Returns:\n",
    "            Экземпляр ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"Загружает Vectorizer из vectorizer_filepath\n",
    "        Args:\n",
    "            vectorizer_filepath (str): путь к Vectorizer\n",
    "        Returns:\n",
    "            Экземпляр Vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "    \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"Сохраняет Vectorizer в json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): путь, куда сохранять\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" Возвращает Vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"Выбирает разбиение, используя колонку split в dataframe\n",
    "        \n",
    "        Args:\n",
    "            split (str): одно из \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Для PyTorch, возвращает длину датасета \"\"\"\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Для PyTorch, возвращает строку по индексу\n",
    "        Args:\n",
    "            index (int): индекс строки данных \n",
    "        Returns:\n",
    "            словарь, содержащий фичи (x_data) и метку (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {\n",
    "            'x_data': review_vector,\n",
    "            'y_target': rating_index\n",
    "        }\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\" Возврращает количество батчей при заданном batch_size\n",
    "        Args:\n",
    "            batch_size (int): размер батча\n",
    "        Returns:\n",
    "            количество батчей в датасете\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(\n",
    "    dataset, \n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"Генератор, использующий PyTorch DataLoader.\n",
    "    Позволяет быть уверенным, что данные будут на одном девайсе.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = dict()\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd62a9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "036c9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" Simple Perceptron classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa044e3e",
   "metadata": {},
   "source": [
    "# Other usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1ffb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {\n",
    "        'stop_early': False,\n",
    "        'early_stopping_step': 0,\n",
    "        'early_stopping_best_val': 1e8,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'epoch_index': 0,\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "        'model_filename': args.model_state_file\n",
    "    }\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # save model at first epoch\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    # if th epoch is not first, update model if results are better\n",
    "    elif train_state['epoch_index'] > 0:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        #if loss got worse\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            \n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "        \n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred) > 0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a18f3f",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adc323c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths \n",
      " ../model_storage/ch3/yelp\\vectorizer.json \n",
      " ../model_storage/ch3/yelp\\model.pth\n",
      "Using CUDA? -> False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data params\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='../data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='../model_storage/ch3/yelp',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # Model params\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime params\n",
    "    catch_keybord_interrupt=True,\n",
    "    cuda=False,\n",
    "    expand_files_to_save_dir=True,\n",
    "    reload_from_files=False\n",
    ")\n",
    "\n",
    "if args.expand_files_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(f\"Expanded filepaths \\n {args.vectorizer_file} \\n {args.model_state_file}\")\n",
    "    \n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "print(f\"Using CUDA? -> {args.cuda}\")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "827fe00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "# Initializings\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34d5a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c3709a33484eebad13a821e58eb2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539a2967aa94c5eb17339bc42d396ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split_train:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55652422cba84a37a8cfaa24557b44c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split_val:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key exiting loop\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_state = make_train_state(args)\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = opt.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = opt.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=1\n",
    ")\n",
    "epoch_bar = tqdm(\n",
    "    desc='training_routine',\n",
    "    total = args.num_epochs,\n",
    "    position=0           \n",
    ")\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(\n",
    "    desc='split_train',\n",
    "    total = dataset.get_num_batches(args.batch_size),\n",
    "    position=1,\n",
    "    leave=True\n",
    ")\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(\n",
    "    desc='split_val',\n",
    "    total = dataset.get_num_batches(args.batch_size),\n",
    "    position=1,\n",
    "    leave=True\n",
    ")\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(\n",
    "            dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            device=args.device\n",
    "        )\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # main train loop\n",
    "            \n",
    "            # step 1: zero grads\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2: compute output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            \n",
    "            # step 3: compute loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # step 4: backprop\n",
    "            loss.backward()\n",
    "            \n",
    "            #step 5: opimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Additional stuff\n",
    "            # accuracy  \n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            #update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # now with validation\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.eval()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = classifier(x_in=batch_dict['x_data']).float()\n",
    "            loss_t = loss_func(y_pred, batch_dict['y_target'].float()).item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "        \n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Key exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcb315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f4254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56cb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
